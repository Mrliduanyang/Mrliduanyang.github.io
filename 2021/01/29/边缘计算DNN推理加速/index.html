<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/blog/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/blog/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/blog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/blog/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="next-config" data-name="main" type="application/json">{&quot;hostname&quot;:&quot;mrliduanyang.github.io&quot;,&quot;root&quot;:&quot;&#x2F;blog&#x2F;&quot;,&quot;images&quot;:&quot;&#x2F;blog&#x2F;images&quot;,&quot;scheme&quot;:&quot;Mist&quot;,&quot;version&quot;:&quot;8.4.0&quot;,&quot;exturl&quot;:false,&quot;sidebar&quot;:{&quot;position&quot;:&quot;left&quot;,&quot;display&quot;:&quot;post&quot;,&quot;padding&quot;:18,&quot;offset&quot;:12},&quot;copycode&quot;:true,&quot;bookmark&quot;:{&quot;enable&quot;:false,&quot;color&quot;:&quot;#222&quot;,&quot;save&quot;:&quot;auto&quot;},&quot;fancybox&quot;:false,&quot;mediumzoom&quot;:false,&quot;lazyload&quot;:false,&quot;pangu&quot;:false,&quot;comments&quot;:{&quot;style&quot;:&quot;tabs&quot;,&quot;active&quot;:null,&quot;storage&quot;:true,&quot;lazyload&quot;:false,&quot;nav&quot;:null},&quot;motion&quot;:{&quot;enable&quot;:true,&quot;async&quot;:false,&quot;transition&quot;:{&quot;post_block&quot;:&quot;fadeIn&quot;,&quot;post_header&quot;:&quot;fadeInDown&quot;,&quot;post_body&quot;:&quot;fadeInDown&quot;,&quot;coll_header&quot;:&quot;fadeInLeft&quot;,&quot;sidebar&quot;:&quot;fadeInUp&quot;}},&quot;prism&quot;:false,&quot;i18n&quot;:{&quot;placeholder&quot;:&quot;搜索...&quot;,&quot;empty&quot;:&quot;没有找到任何搜索结果：${query}&quot;,&quot;hits_time&quot;:&quot;找到 ${hits} 个搜索结果（用时 ${time} 毫秒）&quot;,&quot;hits&quot;:&quot;找到 ${hits} 个搜索结果&quot;}}</script>
<meta name="description" content="移动边缘计算(Multi-Access Edge Computing, MEC)是一种新兴的计算范式，将计算和存储能力“下沉”到网络边缘。越来越多的研究开始尝试在边缘侧部署应用，其中不乏一些计算密集型的深度神经网络(Deep Neural Networks, DNN)应用。但DNN模型的高算力需求和边缘侧设备的弱算力供应之间存在不可调和的矛盾，缓解矛盾的方式大体上有两种：1.优化DNN模型，在不降">
<meta property="og:type" content="article">
<meta property="og:title" content="边缘计算DNN推理加速">
<meta property="og:url" content="https://mrliduanyang.github.io/blog/2021/01/29/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97DNN%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/index.html">
<meta property="og:site_name" content="N.O的个人技术备忘">
<meta property="og:description" content="移动边缘计算(Multi-Access Edge Computing, MEC)是一种新兴的计算范式，将计算和存储能力“下沉”到网络边缘。越来越多的研究开始尝试在边缘侧部署应用，其中不乏一些计算密集型的深度神经网络(Deep Neural Networks, DNN)应用。但DNN模型的高算力需求和边缘侧设备的弱算力供应之间存在不可调和的矛盾，缓解矛盾的方式大体上有两种：1.优化DNN模型，在不降">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224143948.gif">
<meta property="og:image" content="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224144229.gif">
<meta property="og:image" content="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224144310.gif">
<meta property="og:image" content="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224144653.gif">
<meta property="og:image" content="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224144735.png">
<meta property="og:image" content="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224145224.gif">
<meta property="og:image" content="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224145309.png">
<meta property="og:image" content="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224145342.gif">
<meta property="og:image" content="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224145413.png">
<meta property="og:image" content="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224145424.gif">
<meta property="og:image" content="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224145548.png">
<meta property="og:image" content="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224145640.gif">
<meta property="og:image" content="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224145658.png">
<meta property="og:image" content="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224145719.jpg">
<meta property="article:published_time" content="2021-01-29T08:06:16.000Z">
<meta property="article:modified_time" content="2021-05-11T05:09:11.816Z">
<meta property="article:author" content="Mrliduanyang">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224143948.gif">


<link rel="canonical" href="https://mrliduanyang.github.io/blog/2021/01/29/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97DNN%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/">



<script class="next-config" data-name="page" type="application/json">{&quot;sidebar&quot;:&quot;&quot;,&quot;isHome&quot;:false,&quot;isPost&quot;:true,&quot;lang&quot;:&quot;zh-CN&quot;,&quot;comments&quot;:true,&quot;permalink&quot;:&quot;https:&#x2F;&#x2F;mrliduanyang.github.io&#x2F;blog&#x2F;2021&#x2F;01&#x2F;29&#x2F;%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97DNN%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F&#x2F;&quot;,&quot;path&quot;:&quot;2021&#x2F;01&#x2F;29&#x2F;边缘计算DNN推理加速&#x2F;&quot;,&quot;title&quot;:&quot;边缘计算DNN推理加速&quot;}</script>

<script class="next-config" data-name="calendar" type="application/json">&quot;&quot;</script>
<title>边缘计算DNN推理加速 | N.O的个人技术备忘</title><script src="/blog/js/config.js"></script>
  




  <noscript>
    <link rel="stylesheet" href="/blog/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/blog/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">N.O的个人技术备忘</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/blog/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/blog/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-categories"><a href="/blog/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li>
        <li class="menu-item menu-item-commonweal"><a href="/blog/404/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>公益 404</a></li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#0-%E7%A0%94%E7%A9%B6%E5%88%86%E7%B1%BB"><span class="nav-number">1.</span> <span class="nav-text">0.研究分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E6%97%A9%E6%9C%9F%E9%80%80%E5%87%BA"><span class="nav-number">2.</span> <span class="nav-text">1.早期退出</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Adaptive-Parallel-Execution-of-Deep-Neural-Networks-on-Heterogeneous-Edge-Devices-ICDCS-2017"><span class="nav-number">2.1.</span> <span class="nav-text">Adaptive Parallel Execution of Deep Neural Networks on Heterogeneous Edge Devices (ICDCS, 2017)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E6%A8%A1%E5%9E%8B%E5%88%92%E5%88%86-%E6%97%A9%E6%9C%9F%E9%80%80%E5%87%BA"><span class="nav-number">3.</span> <span class="nav-text">2.模型划分+早期退出</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-Edge-AI-On-Demand-Accelerating-Deep-Neural-Network-Inference-via-Edge-Computing-TWC-2020"><span class="nav-number">3.1.</span> <span class="nav-text">2.1 Edge AI: On-Demand Accelerating Deep Neural Network Inference via Edge Computing (TWC, 2020)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-ADDA-Adaptive-Distributed-DNN-Inference-Acceleration-in-Edge-Computing-Environment-ICPADS-2019"><span class="nav-number">3.2.</span> <span class="nav-text">2.2 ADDA: Adaptive Distributed DNN Inference Acceleration in Edge Computing Environment (ICPADS, 2019)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E6%A8%A1%E5%9E%8B%E5%88%92%E5%88%86"><span class="nav-number">4.</span> <span class="nav-text">3.模型划分</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Dynamic-Adaptive-DNN-Surgery-for-Inference-Acceleration-on-the-Edge-INFOCOM-2019"><span class="nav-number">4.1.</span> <span class="nav-text">3.1 Dynamic Adaptive DNN Surgery for Inference Acceleration on the Edge (INFOCOM, 2019)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Towards-Real-time-Cooperative-Deep-Inference-over-the-Cloud-and-Edge-End-Devices-Proceedings-of-the-ACM-on-Interactive-Mobile-Wearable-and-Ubiquitous-Technologies-2020"><span class="nav-number">4.2.</span> <span class="nav-text">3.2 Towards Real-time Cooperative Deep Inference over the Cloud and Edge End Devices (Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 2020)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-Joint-Device-Edge-Inference-over-Wireless-Links-with-Pruning-SPAWC-2020"><span class="nav-number">4.3.</span> <span class="nav-text">3.3 Joint Device-Edge Inference over Wireless Links with Pruning (SPAWC, 2020)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-JointDNN-An-Efficient-Training-and-Inference-Engine-for-Intelligent-Mobile-Cloud-Computing-Services-TMC-2018"><span class="nav-number">4.4.</span> <span class="nav-text">3.4 JointDNN: An Efficient Training and Inference Engine for Intelligent Mobile Cloud Computing Services (TMC, 2018)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-Auto-tuning-Neural-Network-Quantization-Framework-for-Collaborative-Inference-Between-the-Cloud-and-Edge-ICANN-2018"><span class="nav-number">4.5.</span> <span class="nav-text">3.5 Auto-tuning Neural Network Quantization Framework for Collaborative Inference Between the Cloud and Edge (ICANN, 2018)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E5%B1%80%E9%83%A8%E5%B9%B6%E8%A1%8C"><span class="nav-number">5.</span> <span class="nav-text">4.局部并行</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-Fully-Distributed-Deep-Learning-Inference-on-Resource-Constrained-Edge-Devices-SAMOS-2019"><span class="nav-number">5.1.</span> <span class="nav-text">4.1 Fully Distributed Deep Learning Inference on Resource-Constrained Edge Devices (SAMOS, 2019)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-MeDNN-A-distributed-mobile-system-with-enhanced-partition-and-deployment-for-large-scale-DNNs-ICCAD-2017"><span class="nav-number">5.2.</span> <span class="nav-text">4.2 MeDNN: A distributed mobile system with enhanced partition and deployment for large-scale DNNs (ICCAD, 2017)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-Adaptive-parallel-execution-of-deep-neural-networks-on-heterogeneous-edge-device-SEC-2019"><span class="nav-number">5.3.</span> <span class="nav-text">4.3 Adaptive parallel execution of deep neural networks on heterogeneous edge device (SEC, 2019)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-Distributed-Inference-Acceleration-with-Adaptive-DNN-Partitioning-and-Offloading-INFOCOM-2020"><span class="nav-number">5.4.</span> <span class="nav-text">4.4 Distributed Inference Acceleration with Adaptive DNN Partitioning and Offloading (INFOCOM, 2020)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E5%85%A8%E5%B1%80%E5%B9%B6%E8%A1%8C"><span class="nav-number">6.</span> <span class="nav-text">5.全局并行</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-DeepThings-Distributed-Adaptive-Deep-Learning-Inference-on-Resource-Constrained-IoT-Edge-Clusters-IEEE-Transactions-on-Computer-Aided-Design-of-Integrated-Circuits-and-Systems-2018"><span class="nav-number">6.1.</span> <span class="nav-text">5.1 DeepThings: Distributed Adaptive Deep Learning Inference on Resource-Constrained IoT Edge Clusters (IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2018)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-Adaptive-Distributed-Convolutional-Neural-Network-Inference-at-the-Network-Edge-with-ADCNN-ICPP-2020"><span class="nav-number">6.2.</span> <span class="nav-text">5.2 Adaptive Distributed Convolutional Neural Network Inference at the Network Edge with ADCNN (ICPP, 2020)</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Mrliduanyang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/blog/archives">
          <span class="site-state-item-count">18</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/blog/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Mrliduanyang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Mrliduanyang" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:duanyangchn@gmail.com" title="E-Mail → mailto:duanyangchn@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://mrliduanyang.github.io/blog/2021/01/29/%E8%BE%B9%E7%BC%98%E8%AE%A1%E7%AE%97DNN%E6%8E%A8%E7%90%86%E5%8A%A0%E9%80%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="Mrliduanyang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="N.O的个人技术备忘">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          边缘计算DNN推理加速
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-01-29 16:06:16" itemprop="dateCreated datePublished" datetime="2021-01-29T16:06:16+08:00">2021-01-29</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-05-11 13:09:11" itemprop="dateModified" datetime="2021-05-11T13:09:11+08:00">2021-05-11</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/blog/categories/%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93/" itemprop="url" rel="index"><span itemprop="name">论文总结</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>移动边缘计算(Multi-Access Edge Computing, MEC)是一种新兴的计算范式，将计算和存储能力“下沉”到网络边缘。越来越多的研究开始尝试在边缘侧部署应用，其中不乏一些计算密集型的深度神经网络(Deep Neural Networks, DNN)应用。但DNN模型的高算力需求和边缘侧设备的弱算力供应之间存在不可调和的矛盾，缓解矛盾的方式大体上有两种：1.优化DNN模型，在不降低模型性能的前提下减少计算量；2.充分调动边缘侧资源，加速DNN模型在边缘设备上的推理速度。本文就为大家总结了近三年移动边缘计算DNN推理加速的相关研究。</p>
<h2 id="0-研究分类"><a href="#0-研究分类" class="headerlink" title="0.研究分类"></a>0.研究分类</h2><p>按照数据在DNN模型中的处理流程，可以将移动边缘计算DNN推理加速研究分为3大类：早期退出、模型划分、局部并行、全局并行。</p>
<h2 id="1-早期退出"><a href="#1-早期退出" class="headerlink" title="1.早期退出"></a>1.早期退出</h2><h3 id="Adaptive-Parallel-Execution-of-Deep-Neural-Networks-on-Heterogeneous-Edge-Devices-ICDCS-2017"><a href="#Adaptive-Parallel-Execution-of-Deep-Neural-Networks-on-Heterogeneous-Edge-Devices-ICDCS-2017" class="headerlink" title="Adaptive Parallel Execution of Deep Neural Networks on Heterogeneous Edge Devices (ICDCS, 2017)"></a>Adaptive Parallel Execution of Deep Neural Networks on Heterogeneous Edge Devices (ICDCS, 2017)</h3><p><img src="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224143948.gif"></p>
<p>该研究的思想很简单，但也十分有效。研究者认为，DNN模型推理不一定在最后一层才达到最高的精度，有可能在推理到中间某一层的时候，精度已经可以满足设定的阈值，此时推理就可以退出了，该层之后的计算都可以不用进行。极大地减少的计算量，自然整体推理速度就提高了。</p>
<h2 id="2-模型划分-早期退出"><a href="#2-模型划分-早期退出" class="headerlink" title="2.模型划分+早期退出"></a>2.模型划分+早期退出</h2><h3 id="2-1-Edge-AI-On-Demand-Accelerating-Deep-Neural-Network-Inference-via-Edge-Computing-TWC-2020"><a href="#2-1-Edge-AI-On-Demand-Accelerating-Deep-Neural-Network-Inference-via-Edge-Computing-TWC-2020" class="headerlink" title="2.1 Edge AI: On-Demand Accelerating Deep Neural Network Inference via Edge Computing (TWC, 2020)"></a>2.1 Edge AI: On-Demand Accelerating Deep Neural Network Inference via Edge Computing (TWC, 2020)</h3><p><img src="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224144229.gif"></p>
<p>该研究的关键点有两个：1.DNN划分，根据可用带宽在移动设备和边缘服务器之间自适应地对DNN计算进行划分，以便利用边缘服务器的计算能力；2.移动设备上模型早期退出，因为DNN划分后的执行性能仍然受到在移动设备上运行的模型的制约，可以通过早期退出中间DNN层的推理来加速整体推理。通过对DNN分区和 退出点调整进行联合优化，可以最大限度地提高准确性，同时保证延迟要求。</p>
<h3 id="2-2-ADDA-Adaptive-Distributed-DNN-Inference-Acceleration-in-Edge-Computing-Environment-ICPADS-2019"><a href="#2-2-ADDA-Adaptive-Distributed-DNN-Inference-Acceleration-in-Edge-Computing-Environment-ICPADS-2019" class="headerlink" title="2.2 ADDA: Adaptive Distributed DNN Inference Acceleration in Edge Computing Environment (ICPADS, 2019)"></a>2.2 ADDA: Adaptive Distributed DNN Inference Acceleration in Edge Computing Environment (ICPADS, 2019)</h3><p><img src="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224144310.gif"></p>
<p>该研究考虑了边缘计算执行框架优化的两个方面：1.DNN计算路径优化，构造多个DNN计算路径，在精度保证下实现不同输入数据推理的早期退出；2.DNN计算划分优化，考虑到边缘服务器的负载、网络带宽和延迟，对具有不同层划分的多路径DNN网络的总推理执行时间进行了分析和建模，可以实现终端设备和边缘服务器之间的动态最佳DNN分区，以进一步降低推理延迟。</p>
<h2 id="3-模型划分"><a href="#3-模型划分" class="headerlink" title="3.模型划分"></a>3.模型划分</h2><h3 id="3-1-Dynamic-Adaptive-DNN-Surgery-for-Inference-Acceleration-on-the-Edge-INFOCOM-2019"><a href="#3-1-Dynamic-Adaptive-DNN-Surgery-for-Inference-Acceleration-on-the-Edge-INFOCOM-2019" class="headerlink" title="3.1 Dynamic Adaptive DNN Surgery for Inference Acceleration on the Edge (INFOCOM, 2019)"></a>3.1 Dynamic Adaptive DNN Surgery for Inference Acceleration on the Edge (INFOCOM, 2019)</h3><p><img src="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224144653.gif"></p>
<p>该研究关注边缘侧和云的结合，认为DNN不同层的划分会导致不同的计算时间和传输时间，DNN的划分实际上是一种计算和传输之间的权衡，因此是存在最佳划分的。设计了动态自适应DNN划分方案，通过持续监控网络状况来优化DNN网络划分，以寻找具有动态网络条件的集成边缘和云计算环境中的最佳DNN分区。</p>
<h3 id="3-2-Towards-Real-time-Cooperative-Deep-Inference-over-the-Cloud-and-Edge-End-Devices-Proceedings-of-the-ACM-on-Interactive-Mobile-Wearable-and-Ubiquitous-Technologies-2020"><a href="#3-2-Towards-Real-time-Cooperative-Deep-Inference-over-the-Cloud-and-Edge-End-Devices-Proceedings-of-the-ACM-on-Interactive-Mobile-Wearable-and-Ubiquitous-Technologies-2020" class="headerlink" title="3.2 Towards Real-time Cooperative Deep Inference over the Cloud and Edge End Devices (Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 2020)"></a>3.2 Towards Real-time Cooperative Deep Inference over the Cloud and Edge End Devices (Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 2020)</h3><p><img src="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224144735.png"></p>
<p>该研究将DNN模型自适应划分为两部分，并在不同的设备上执行不同的部分，以最小化总推理延迟。研究者把最优DNN划分转化为有向无环图(DAG)中的最小割问题，并提出了一种新的两阶段快速深度模型划分(QDMP)方法来求解它。</p>
<h3 id="3-3-Joint-Device-Edge-Inference-over-Wireless-Links-with-Pruning-SPAWC-2020"><a href="#3-3-Joint-Device-Edge-Inference-over-Wireless-Links-with-Pruning-SPAWC-2020" class="headerlink" title="3.3 Joint Device-Edge Inference over Wireless Links with Pruning (SPAWC, 2020)"></a>3.3 Joint Device-Edge Inference over Wireless Links with Pruning (SPAWC, 2020)</h3><p><img src="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224145224.gif"></p>
<p>该研究考虑了结合网络剪枝，在给定信道条件和物联网设备计算约束条件下，找到最佳的DNN划分点和剪枝参数，以确保在给定场景下尽可能达到最佳的精度和效率。</p>
<h3 id="3-4-JointDNN-An-Efficient-Training-and-Inference-Engine-for-Intelligent-Mobile-Cloud-Computing-Services-TMC-2018"><a href="#3-4-JointDNN-An-Efficient-Training-and-Inference-Engine-for-Intelligent-Mobile-Cloud-Computing-Services-TMC-2018" class="headerlink" title="3.4 JointDNN: An Efficient Training and Inference Engine for Intelligent Mobile Cloud Computing Services (TMC, 2018)"></a>3.4 JointDNN: An Efficient Training and Inference Engine for Intelligent Mobile Cloud Computing Services (TMC, 2018)</h3><p>该研究关注DNN在移动设备和云的联合平台中推理和训练，将DNN结构建模为一个有向无环图(DAG)，证明了在不同情况下，即最佳性能或能量消耗的最优计算调度问题，可以归结为多项式时间多种资源约束下的最短路径问题。</p>
<p><img src="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224145309.png"></p>
<h3 id="3-5-Auto-tuning-Neural-Network-Quantization-Framework-for-Collaborative-Inference-Between-the-Cloud-and-Edge-ICANN-2018"><a href="#3-5-Auto-tuning-Neural-Network-Quantization-Framework-for-Collaborative-Inference-Between-the-Cloud-and-Edge-ICANN-2018" class="headerlink" title="3.5 Auto-tuning Neural Network Quantization Framework for Collaborative Inference Between the Cloud and Edge (ICANN, 2018)"></a>3.5 Auto-tuning Neural Network Quantization Framework for Collaborative Inference Between the Cloud and Edge (ICANN, 2018)</h3><p>该研究对DNN的算子进行剖面，并生成候选图层作为划分点。当使用神经网络时，框架将自动调整网络划分。推理时，网络的第一部分在边缘设备上量化和执行以减少存储和计算，网络的第二部分全精度网络在云服务器中执行。</p>
<p><img src="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224145342.gif"></p>
<h2 id="4-局部并行"><a href="#4-局部并行" class="headerlink" title="4.局部并行"></a>4.局部并行</h2><h3 id="4-1-Fully-Distributed-Deep-Learning-Inference-on-Resource-Constrained-Edge-Devices-SAMOS-2019"><a href="#4-1-Fully-Distributed-Deep-Learning-Inference-on-Resource-Constrained-Edge-Devices-SAMOS-2019" class="headerlink" title="4.1 Fully Distributed Deep Learning Inference on Resource-Constrained Edge Devices (SAMOS, 2019)"></a>4.1 Fully Distributed Deep Learning Inference on Resource-Constrained Edge Devices (SAMOS, 2019)</h3><p><img src="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224145413.png" alt="484168_1_En_6_Fig6_HTML"></p>
<p>该研究考虑了DNN模型划分后的数据通信开销，优化了覆盖DNN所有层分布式执行的存储、计算和通信需求。对于给定数量的边缘设备，该方案是联合应用的，以最小化设备之间的交换数据量，优化运行时间。</p>
<h3 id="4-2-MeDNN-A-distributed-mobile-system-with-enhanced-partition-and-deployment-for-large-scale-DNNs-ICCAD-2017"><a href="#4-2-MeDNN-A-distributed-mobile-system-with-enhanced-partition-and-deployment-for-large-scale-DNNs-ICCAD-2017" class="headerlink" title="4.2 MeDNN: A distributed mobile system with enhanced partition and deployment for large-scale DNNs (ICCAD, 2017)"></a>4.2 MeDNN: A distributed mobile system with enhanced partition and deployment for large-scale DNNs (ICCAD, 2017)</h3><p><img src="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224145424.gif" alt="8203852-fig-3-source-large"></p>
<p>该研究将MapReduce的概念用到分布式体系结构中，由Group Owner对特征图进行划分，并由每个Worker Node进行卷积计算，计算结果再由Group Owner进行归并以进行下一次计算。</p>
<h3 id="4-3-Adaptive-parallel-execution-of-deep-neural-networks-on-heterogeneous-edge-device-SEC-2019"><a href="#4-3-Adaptive-parallel-execution-of-deep-neural-networks-on-heterogeneous-edge-device-SEC-2019" class="headerlink" title="4.3 Adaptive parallel execution of deep neural networks on heterogeneous edge device (SEC, 2019)"></a>4.3 Adaptive parallel execution of deep neural networks on heterogeneous edge device (SEC, 2019)</h3><p><img src="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224145548.png" alt="image-20201217164706537"></p>
<p>该研究使用预测模型来预测DNN每一层在设备上的执行时间，并为每个划分选择最佳的划分点和并行化策略。</p>
<h3 id="4-4-Distributed-Inference-Acceleration-with-Adaptive-DNN-Partitioning-and-Offloading-INFOCOM-2020"><a href="#4-4-Distributed-Inference-Acceleration-with-Adaptive-DNN-Partitioning-and-Offloading-INFOCOM-2020" class="headerlink" title="4.4 Distributed Inference Acceleration with Adaptive DNN Partitioning and Offloading (INFOCOM, 2020)"></a>4.4 Distributed Inference Acceleration with Adaptive DNN Partitioning and Offloading (INFOCOM, 2020)</h3><p>该研究提出了一个细粒度的自适应划分方案，将DNN分成比单个图层更小的部分从而减少网络中的通信开销。使用一种基于交换匹配的分布式算法将DNN推理从端设备卸载到雾网络中的节点。</p>
<p><img src="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224145640.gif"></p>
<h2 id="5-全局并行"><a href="#5-全局并行" class="headerlink" title="5.全局并行"></a>5.全局并行</h2><h3 id="5-1-DeepThings-Distributed-Adaptive-Deep-Learning-Inference-on-Resource-Constrained-IoT-Edge-Clusters-IEEE-Transactions-on-Computer-Aided-Design-of-Integrated-Circuits-and-Systems-2018"><a href="#5-1-DeepThings-Distributed-Adaptive-Deep-Learning-Inference-on-Resource-Constrained-IoT-Edge-Clusters-IEEE-Transactions-on-Computer-Aided-Design-of-Integrated-Circuits-and-Systems-2018" class="headerlink" title="5.1 DeepThings: Distributed Adaptive Deep Learning Inference on Resource-Constrained IoT Edge Clusters (IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2018)"></a>5.1 DeepThings: Distributed Adaptive Deep Learning Inference on Resource-Constrained IoT Edge Clusters (IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2018)</h3><p><img src="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224145658.png"></p>
<p>该研究重点讨论能够执行早期卷积层的划分和分布方法，关键思想是将原始的CNN层分割成独立的可分配执行单元，DeepThings将动态平衡边缘集群之间的工作负载。</p>
<h3 id="5-2-Adaptive-Distributed-Convolutional-Neural-Network-Inference-at-the-Network-Edge-with-ADCNN-ICPP-2020"><a href="#5-2-Adaptive-Distributed-Convolutional-Neural-Network-Inference-at-the-Network-Edge-with-ADCNN-ICPP-2020" class="headerlink" title="5.2 Adaptive Distributed Convolutional Neural Network Inference at the Network Edge with ADCNN (ICPP, 2020)"></a>5.2 Adaptive Distributed Convolutional Neural Network Inference at the Network Edge with ADCNN (ICPP, 2020)</h3><p><img src="https://gitee.com/molinchn/BlogImage/raw/master/duanyang/20201224145719.jpg"></p>
<p>该研究提出了一种在动态边缘计算环境中运行CNN推理任务的新方法，将卷积层划分为许多小型的独立计算任务，根据边缘节点的当前条件动态地将分区任务分配给边缘节点，以进行并行处理，还提出一个压缩方案，进一步最大限度地降低跨层的通信成本。</p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/blog/2021/01/29/%E9%80%92%E5%BD%92%E5%86%99%E6%B3%95/" rel="prev" title="递归问题解题技巧">
                  <i class="fa fa-chevron-left"></i> 递归问题解题技巧
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/blog/2021/01/29/%E6%AF%8F%E5%A4%A9%E4%B8%80%E4%B8%AAMMDetection%E5%B0%8F%E6%8A%80%E5%B7%A7/" rel="next" title="每天一个MMDetection小技巧">
                  每天一个MMDetection小技巧 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>





<script src="/blog/js/comments.js"></script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">津ICP备20002882号 </a>
  </div>

<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mrliduanyang</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/mist/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/blog/js/utils.js"></script><script src="/blog/js/motion.js"></script><script src="/blog/js/schemes/muse.js"></script><script src="/blog/js/next-boot.js"></script>

  






  





</body>
</html>
